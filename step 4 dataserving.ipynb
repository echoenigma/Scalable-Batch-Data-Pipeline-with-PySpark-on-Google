{"cells": [{"cell_type": "code", "execution_count": 3, "id": "61fc3142-9ac3-4a63-9f02-903ad0163558", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/04/08 07:28:59 INFO SparkEnv: Registering MapOutputTracker\n25/04/08 07:28:59 INFO SparkEnv: Registering BlockManagerMaster\n25/04/08 07:28:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n25/04/08 07:29:00 INFO SparkEnv: Registering OutputCommitCoordinator\n"}], "source": "from pyspark.sql import SparkSession\n\n# Create basic SparkSession\nspark = SparkSession.builder \\\n    .appName(\"DataServingLayer\") \\\n    .getOrCreate()\n"}, {"cell_type": "code", "execution_count": 4, "id": "0ff48769-4b22-4dc0-93c6-ae22f0dd603a", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "full_order_df=spark.read.parquet(\"/data/olist/processed\")"}, {"cell_type": "code", "execution_count": 6, "id": "bbe48304-b2b6-474d-8fd8-4ae70c775bde", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- customer_id: string (nullable = true)\n |-- review_id: string (nullable = true)\n |-- order_id: string (nullable = true)\n |-- seller_id: string (nullable = true)\n |-- product_id: string (nullable = true)\n |-- order_status: string (nullable = true)\n |-- order_purchase_timestamp: timestamp (nullable = true)\n |-- order_approved_at: timestamp (nullable = true)\n |-- order_delivered_carrier_date: timestamp (nullable = true)\n |-- order_delivered_customer_date: timestamp (nullable = true)\n |-- order_estimated_delivery_date: timestamp (nullable = true)\n |-- order_item_id: integer (nullable = true)\n |-- shipping_limit_date: timestamp (nullable = true)\n |-- price: double (nullable = true)\n |-- freight_value: double (nullable = true)\n |-- product_category_name: string (nullable = true)\n |-- product_name_lenght: integer (nullable = true)\n |-- product_description_lenght: integer (nullable = true)\n |-- product_photos_qty: integer (nullable = true)\n |-- product_weight_g: integer (nullable = true)\n |-- product_length_cm: integer (nullable = true)\n |-- product_height_cm: integer (nullable = true)\n |-- product_width_cm: integer (nullable = true)\n |-- seller_zip_code_prefix: integer (nullable = true)\n |-- seller_city: string (nullable = true)\n |-- seller_state: string (nullable = true)\n |-- customer_unique_id: string (nullable = true)\n |-- customer_zip_code_prefix: integer (nullable = true)\n |-- customer_city: string (nullable = true)\n |-- customer_state: string (nullable = true)\n |-- geolocation_zip_code_prefix: integer (nullable = true)\n |-- geolocation_lat: double (nullable = true)\n |-- geolocation_lng: double (nullable = true)\n |-- geolocation_city: string (nullable = true)\n |-- geolocation_state: string (nullable = true)\n |-- review_comment_title: string (nullable = true)\n |-- review_comment_message: string (nullable = true)\n |-- review_creation_date: string (nullable = true)\n |-- review_answer_timestamp: string (nullable = true)\n |-- payment_sequential: integer (nullable = true)\n |-- payment_type: string (nullable = true)\n |-- payment_installments: integer (nullable = true)\n |-- payment_value: double (nullable = true)\n |-- review_score: string (nullable = true)\n |-- is_delivered: integer (nullable = true)\n |-- is_cancelled: integer (nullable = true)\n |-- order_revenue: double (nullable = true)\n |-- customer_segment: string (nullable = true)\n |-- order_day_type: string (nullable = true)\n\n"}], "source": "full_order_df.printSchema()"}, {"cell_type": "code", "execution_count": 7, "id": "9f3acb7b-2261-4ba0-bf00-50af1e3a43aa", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/04/08 07:35:36 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n                                                                                \r"}], "source": "bucket_path = \"gs://dataproc-staging-us-central1-857525592031-u0kamy9k/temp_data/\"\nfull_order_df.write \\\n    .mode(\"overwrite\") \\\n    .parquet(bucket_path)"}, {"cell_type": "code", "execution_count": null, "id": "0dad2b1b-b933-4aad-927c-45a98f359ebd", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}