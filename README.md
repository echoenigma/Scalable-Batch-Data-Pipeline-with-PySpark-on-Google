# Scalable-Batch-Data-Pipeline-with-PySpark-on-Google
Built a batch data pipeline to clean, transform, and integrate Olist e-commerce data using PySpark on Google Cloud Dataproc for scalable cloud-based processing.

# Olist Data Pipeline with PySpark on GCP

## Project Overview

This project showcases a batch data pipeline built with PySpark on Google Cloud Platform (GCP) Dataproc. It processes the Olist e-commerce dataset by performing data ingestion, cleaning, transformation, and integration to prepare the data for further analysis. The pipeline demonstrates scalable and efficient data engineering practices using cloud resources.

## Key Features

- Data ingestion of raw CSV files from cloud storage  
- Data cleaning and transformation using PySpark  
- Integration of multiple datasets for unified analytics  
- Storage of processed data for downstream usage  
- Cloud-based execution on GCP Dataproc clusters  

## Technologies Used

- Apache PySpark  
- Google Cloud Platform (Dataproc, Cloud Storage)  
- JupyterLab  
- Python  



